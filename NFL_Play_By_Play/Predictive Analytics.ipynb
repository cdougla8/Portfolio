{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Predicting the Next Play\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Imports"
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": "from metaflow import Flow\nimport pandas\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import model_selection\nimport xgboost\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import roc_auc_score\nrun = Flow('NFLStatsFlow').latest_successful_run"
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>play_id</th>\n      <th>play_id</th>\n    </tr>\n    <tr>\n      <th>full_play_type</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>pass_left_deep</th>\n      <td>0.028939</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>pass_left_short</th>\n      <td>0.188639</td>\n      <td>176</td>\n    </tr>\n    <tr>\n      <th>pass_middle_deep</th>\n      <td>0.023580</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>pass_middle_short</th>\n      <td>0.132905</td>\n      <td>124</td>\n    </tr>\n    <tr>\n      <th>pass_right_deep</th>\n      <td>0.027867</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>pass_right_short</th>\n      <td>0.163987</td>\n      <td>153</td>\n    </tr>\n    <tr>\n      <th>run_left</th>\n      <td>0.160772</td>\n      <td>150</td>\n    </tr>\n    <tr>\n      <th>run_middle</th>\n      <td>0.115756</td>\n      <td>108</td>\n    </tr>\n    <tr>\n      <th>run_right</th>\n      <td>0.157556</td>\n      <td>147</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                    play_id  play_id\nfull_play_type                      \npass_left_deep     0.028939       27\npass_left_short    0.188639      176\npass_middle_deep   0.023580       22\npass_middle_short  0.132905      124\npass_right_deep    0.027867       26\npass_right_short   0.163987      153\nrun_left           0.160772      150\nrun_middle         0.115756      108\nrun_right          0.157556      147"
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "new_df = run.data.san_fran_df.groupby(['full_play_type'])['play_id'].count().to_frame()\npercentage_df = new_df.apply(lambda x: x/x.sum())\ntotals_df = pandas.concat([percentage_df, new_df], axis = 1)\ntotals_df"
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Iteration 1, loss = 18.91155499\nIteration 2, loss = 14.17241692\nIteration 3, loss = 2.78951859\nIteration 4, loss = 2.48461881\nIteration 5, loss = 2.27875134\nIteration 6, loss = 2.28718508\nIteration 7, loss = 2.23945491\nIteration 8, loss = 2.28549019\nIteration 9, loss = 2.23704335\nIteration 10, loss = 2.24917363\nIteration 11, loss = 2.21439380\nIteration 12, loss = 2.20292680\nIteration 13, loss = 2.19103194\nIteration 14, loss = 2.18537340\nIteration 15, loss = 2.18350949\nIteration 16, loss = 2.17679189\nIteration 17, loss = 2.17199016\nIteration 18, loss = 2.16781012\nIteration 19, loss = 2.16381982\nIteration 20, loss = 2.16035464\nIteration 21, loss = 2.15707647\nIteration 22, loss = 2.15417421\nIteration 23, loss = 2.15140114\nIteration 24, loss = 2.14867942\nIteration 25, loss = 2.14583616\nIteration 26, loss = 2.14335836\nIteration 27, loss = 2.14057172\nIteration 28, loss = 2.13792825\nIteration 29, loss = 2.13552894\nIteration 30, loss = 2.13303740\nIteration 31, loss = 2.13080816\nIteration 32, loss = 2.12837867\nIteration 33, loss = 2.12600983\nIteration 34, loss = 2.12377328\nIteration 35, loss = 2.12169995\nIteration 36, loss = 2.11973359\nIteration 37, loss = 2.11789501\nIteration 38, loss = 2.11615458\nIteration 39, loss = 2.11417087\nIteration 40, loss = 2.11235922\nIteration 41, loss = 2.11058584\nIteration 42, loss = 2.10883780\nIteration 43, loss = 2.10735584\nIteration 44, loss = 2.10564369\nIteration 45, loss = 2.10408999\nIteration 46, loss = 2.10258968\nIteration 47, loss = 2.10097437\nIteration 48, loss = 2.09950000\nIteration 49, loss = 2.09799709\nIteration 50, loss = 2.09668282\nIteration 51, loss = 2.09520268\nIteration 52, loss = 2.09380066\nIteration 53, loss = 2.09237159\nIteration 54, loss = 2.09102864\nIteration 55, loss = 2.08966715\nIteration 56, loss = 2.08836365\nIteration 57, loss = 2.08708844\nIteration 58, loss = 2.08611087\nIteration 59, loss = 2.08472397\nIteration 60, loss = 2.08333379\nIteration 61, loss = 2.08229813\nIteration 62, loss = 2.08120256\nIteration 63, loss = 2.07982150\nIteration 64, loss = 2.07863114\nIteration 65, loss = 2.07752780\nIteration 66, loss = 2.07657006\nIteration 67, loss = 2.07531129\nIteration 68, loss = 2.07414923\nIteration 69, loss = 2.07319457\nIteration 70, loss = 2.07207163\nIteration 71, loss = 2.07112296\nIteration 72, loss = 2.07013054\nIteration 73, loss = 2.06927037\nIteration 74, loss = 2.06824220\nIteration 75, loss = 2.06712153\nIteration 76, loss = 2.06620079\nIteration 77, loss = 2.06533387\nIteration 78, loss = 2.06457928\nIteration 79, loss = 2.06344401\nIteration 80, loss = 2.06254311\nIteration 81, loss = 2.06180417\nIteration 82, loss = 2.06078825\nIteration 83, loss = 2.06006677\nIteration 84, loss = 2.05914365\nIteration 85, loss = 2.05842984\nIteration 86, loss = 2.05745663\nIteration 87, loss = 2.05660762\nIteration 88, loss = 2.05582053\nIteration 89, loss = 2.05492217\nIteration 90, loss = 2.05409951\nIteration 91, loss = 2.05331609\nIteration 92, loss = 2.05280411\nIteration 93, loss = 2.05200630\nIteration 94, loss = 2.05128038\nIteration 95, loss = 2.05030843\nIteration 96, loss = 2.04967464\nIteration 97, loss = 2.04888139\nIteration 98, loss = 2.04816053\nIteration 99, loss = 2.04739539\nIteration 100, loss = 2.04672910\nIteration 101, loss = 2.04635407\nIteration 102, loss = 2.04551871\nIteration 103, loss = 2.04497066\nIteration 104, loss = 2.04423018\nIteration 105, loss = 2.04338943\nIteration 106, loss = 2.04289041\nIteration 107, loss = 2.04220745\nIteration 108, loss = 2.04158086\nIteration 109, loss = 2.04103483\nIteration 110, loss = 2.04045263\nIteration 111, loss = 2.04010831\nIteration 112, loss = 2.03923114\nIteration 113, loss = 2.03891133\nIteration 114, loss = 2.03800760\nIteration 115, loss = 2.03757842\nIteration 116, loss = 2.03687369\nIteration 117, loss = 2.03644103\nIteration 118, loss = 2.03600360\nIteration 119, loss = 2.03537707\nIteration 120, loss = 2.03480888\nIteration 121, loss = 2.03441845\nIteration 122, loss = 2.03391723\nIteration 123, loss = 2.03337050\nIteration 124, loss = 2.03289479\nIteration 125, loss = 2.03245452\nIteration 126, loss = 2.03189080\nIteration 127, loss = 2.03119436\nIteration 128, loss = 2.03065918\nIteration 129, loss = 2.03037936\nIteration 130, loss = 2.02984713\nIteration 131, loss = 2.02944819\nIteration 132, loss = 2.02896494\nIteration 133, loss = 2.02850530\nIteration 134, loss = 2.02822694\nIteration 135, loss = 2.02763273\nIteration 136, loss = 2.02717308\nIteration 137, loss = 2.02669646\nIteration 138, loss = 2.02650318\nIteration 139, loss = 2.02623365\nIteration 140, loss = 2.02549612\nIteration 141, loss = 2.02514664\nIteration 142, loss = 2.02482357\nIteration 143, loss = 2.02444848\nIteration 144, loss = 2.02401316\nIteration 145, loss = 2.02363120\nIteration 146, loss = 2.02310566\nIteration 147, loss = 2.02327649\nIteration 148, loss = 2.02224132\nIteration 149, loss = 2.02283167\nIteration 150, loss = 2.02157293\nIteration 151, loss = 2.02149952\nIteration 152, loss = 2.02082838\nIteration 153, loss = 2.02081530\nIteration 154, loss = 2.02006767\nIteration 155, loss = 2.01983451\nIteration 156, loss = 2.01960183\nIteration 157, loss = 2.01921746\nIteration 158, loss = 2.01883356\nIteration 159, loss = 2.01850191\nIteration 160, loss = 2.01825330\nIteration 161, loss = 2.01785318\nIteration 162, loss = 2.01790441\nIteration 163, loss = 2.01735892\nIteration 164, loss = 2.01714848\nIteration 165, loss = 2.01681002\nIteration 166, loss = 2.01627159\nIteration 167, loss = 2.01606051\nIteration 168, loss = 2.01609149\nIteration 169, loss = 2.01588780\nIteration 170, loss = 2.01535018\nIteration 171, loss = 2.01505299\nIteration 172, loss = 2.01468047\nIteration 173, loss = 2.01449699\nIteration 174, loss = 2.01425631\nIteration 175, loss = 2.01391567\nIteration 176, loss = 2.01372652\nIteration 177, loss = 2.01361871\nIteration 178, loss = 2.01315026\nIteration 179, loss = 2.01281680\nIteration 180, loss = 2.01254436\nIteration 181, loss = 2.01224551\nIteration 182, loss = 2.01211098\nIteration 183, loss = 2.01207269\nIteration 184, loss = 2.01166529\nIteration 185, loss = 2.01160801\nIteration 186, loss = 2.01126758\nIteration 187, loss = 2.01086247\nIteration 188, loss = 2.01084753\nIteration 189, loss = 2.01057315\nIteration 190, loss = 2.01033846\nIteration 191, loss = 2.01005646\nIteration 192, loss = 2.00978462\nIteration 193, loss = 2.00993989\nIteration 194, loss = 2.00957347\nIteration 195, loss = 2.00922956\nIteration 196, loss = 2.00911891\nIteration 197, loss = 2.00915052\nIteration 198, loss = 2.00901393\nIteration 199, loss = 2.00865328\nIteration 200, loss = 2.00806865\nIteration 201, loss = 2.00829022\nIteration 202, loss = 2.00957714\nIteration 203, loss = 2.00879617\nIteration 204, loss = 2.00856849\nIteration 205, loss = 2.00696644\nIteration 206, loss = 2.00831180\nIteration 207, loss = 2.00744111\nIteration 208, loss = 2.00671369\nIteration 209, loss = 2.00675127\nIteration 210, loss = 2.00650138\nIteration 211, loss = 2.00632824\nIteration 212, loss = 2.00608960\nIteration 213, loss = 2.00583593\nIteration 214, loss = 2.00589521\nIteration 215, loss = 2.00577609\nIteration 216, loss = 2.00550008\nIteration 217, loss = 2.00535634\nIteration 218, loss = 2.00531113\nIteration 219, loss = 2.00508146\nIteration 220, loss = 2.00506303\nIteration 221, loss = 2.00525200\nIteration 222, loss = 2.00476108\nIteration 223, loss = 2.00468446\nIteration 224, loss = 2.00425686\nIteration 225, loss = 2.00427691\nIteration 226, loss = 2.00406061\nIteration 227, loss = 2.00372108\nIteration 228, loss = 2.00386314\nIteration 229, loss = 2.00386201\nIteration 230, loss = 2.00351901\nIteration 231, loss = 2.00339776\nIteration 232, loss = 2.00299954\nIteration 233, loss = 2.00333159\nIteration 234, loss = 2.00295177\nIteration 235, loss = 2.00275384\nIteration 236, loss = 2.00288070\nIteration 237, loss = 2.00281726\nIteration 238, loss = 2.00246543\nIteration 239, loss = 2.00230842\nIteration 240, loss = 2.00246620\nIteration 241, loss = 2.00201845\nIteration 242, loss = 2.00198085\nIteration 243, loss = 2.00214697\nIteration 244, loss = 2.00205115\nIteration 245, loss = 2.00178422\nIteration 246, loss = 2.00154190\nIteration 247, loss = 2.00149872\nIteration 248, loss = 2.00170980\nIteration 249, loss = 2.00147387\nIteration 250, loss = 2.00120019\nIteration 251, loss = 2.00111998\nIteration 252, loss = 2.00117161\nIteration 253, loss = 2.00085272\nIteration 254, loss = 2.00096886\nIteration 255, loss = 2.00092859\nIteration 256, loss = 2.00109920\nIteration 257, loss = 2.00093603\nIteration 258, loss = 2.00047106\nIteration 259, loss = 2.00034490\nIteration 260, loss = 2.00030646\nIteration 261, loss = 2.00033255\nIteration 262, loss = 2.00092953\nIteration 263, loss = 2.00007184\nIteration 264, loss = 1.99982626\nIteration 265, loss = 1.99986389\nIteration 266, loss = 1.99974075\nIteration 267, loss = 1.99984307\nIteration 268, loss = 1.99988711\nIteration 269, loss = 1.99953270\nIteration 270, loss = 1.99928074\nIteration 271, loss = 1.99898489\nIteration 272, loss = 1.99908494\nIteration 273, loss = 1.99912153\nIteration 274, loss = 1.99921886\nIteration 275, loss = 1.99897456\nIteration 276, loss = 1.99888321\nIteration 277, loss = 1.99866614\nIteration 278, loss = 1.99898646\nIteration 279, loss = 1.99864249\nIteration 280, loss = 1.99870897\nIteration 281, loss = 1.99842672\nIteration 282, loss = 1.99820603\nIteration 283, loss = 1.99810989\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Iteration 284, loss = 1.99795768\nIteration 285, loss = 1.99826771\nIteration 286, loss = 1.99808592\nIteration 287, loss = 1.99774547\nIteration 288, loss = 1.99776840\nIteration 289, loss = 1.99766471\nIteration 290, loss = 1.99781266\nIteration 291, loss = 1.99779843\nIteration 292, loss = 1.99751927\nIteration 293, loss = 1.99752086\nIteration 294, loss = 1.99738350\nIteration 295, loss = 1.99732761\nIteration 296, loss = 1.99733533\nIteration 297, loss = 1.99741825\nIteration 298, loss = 1.99722369\nIteration 299, loss = 1.99730775\nIteration 300, loss = 1.99719386\nIteration 301, loss = 1.99723238\nIteration 302, loss = 1.99692806\nIteration 303, loss = 1.99692206\nIteration 304, loss = 1.99682756\nIteration 305, loss = 1.99704314\nIteration 306, loss = 1.99661342\nIteration 307, loss = 1.99672520\nIteration 308, loss = 1.99684710\nIteration 309, loss = 1.99664658\nIteration 310, loss = 1.99685101\nIteration 311, loss = 1.99642507\nIteration 312, loss = 1.99645005\nIteration 313, loss = 1.99644505\nIteration 314, loss = 1.99656216\nIteration 315, loss = 1.99648183\nIteration 316, loss = 1.99618573\nIteration 317, loss = 1.99656635\nIteration 318, loss = 1.99625939\nIteration 319, loss = 1.99599758\nIteration 320, loss = 1.99593698\nIteration 321, loss = 1.99602093\nIteration 322, loss = 1.99582832\nIteration 323, loss = 1.99591314\nIteration 324, loss = 1.99586364\nIteration 325, loss = 1.99589105\nIteration 326, loss = 1.99592932\nIteration 327, loss = 1.99559008\nIteration 328, loss = 1.99574689\nIteration 329, loss = 1.99547433\nIteration 330, loss = 1.99556423\nIteration 331, loss = 1.99570676\nIteration 332, loss = 1.99536867\nIteration 333, loss = 1.99547289\nIteration 334, loss = 1.99543198\nIteration 335, loss = 1.99535993\nIteration 336, loss = 1.99555234\nIteration 337, loss = 1.99520938\nIteration 338, loss = 1.99523812\nIteration 339, loss = 1.99553657\nIteration 340, loss = 1.99552711\nIteration 341, loss = 1.99509241\nIteration 342, loss = 1.99511587\nIteration 343, loss = 1.99499565\nIteration 344, loss = 1.99499476\nIteration 345, loss = 1.99505988\nIteration 346, loss = 1.99487079\nIteration 347, loss = 1.99547867\nIteration 348, loss = 1.99557447\nIteration 349, loss = 1.99490740\nIteration 350, loss = 1.99482269\nIteration 351, loss = 1.99493432\nIteration 352, loss = 1.99479065\nIteration 353, loss = 1.99480904\nIteration 354, loss = 1.99466540\nIteration 355, loss = 1.99492751\nIteration 356, loss = 1.99472962\nIteration 357, loss = 1.99459981\nIteration 358, loss = 1.99461834\nIteration 359, loss = 1.99450087\nIteration 360, loss = 1.99446403\nIteration 361, loss = 1.99454514\nIteration 362, loss = 1.99476410\nIteration 363, loss = 1.99465458\nIteration 364, loss = 1.99447247\nIteration 365, loss = 1.99439963\nIteration 366, loss = 1.99433763\nIteration 367, loss = 1.99448300\nIteration 368, loss = 1.99421743\nIteration 369, loss = 1.99455758\nIteration 370, loss = 1.99413040\nIteration 371, loss = 1.99459725\nIteration 372, loss = 1.99411256\nIteration 373, loss = 1.99415605\nIteration 374, loss = 1.99426615\nIteration 375, loss = 1.99441013\nIteration 376, loss = 1.99416110\nIteration 377, loss = 1.99423761\nIteration 378, loss = 1.99418004\nIteration 379, loss = 1.99394569\nIteration 380, loss = 1.99463081\nIteration 381, loss = 1.99412133\nIteration 382, loss = 1.99423128\nIteration 383, loss = 1.99396173\nIteration 384, loss = 1.99409030\nIteration 385, loss = 1.99394278\nIteration 386, loss = 1.99390768\nIteration 387, loss = 1.99396572\nIteration 388, loss = 1.99375093\nIteration 389, loss = 1.99394605\nIteration 390, loss = 1.99393024\nIteration 391, loss = 1.99365671\nIteration 392, loss = 1.99403417\nIteration 393, loss = 1.99372659\nIteration 394, loss = 1.99370286\nIteration 395, loss = 1.99385782\nIteration 396, loss = 1.99377370\nIteration 397, loss = 1.99378906\nIteration 398, loss = 1.99401071\nIteration 399, loss = 1.99389018\nIteration 400, loss = 1.99362423\nIteration 401, loss = 1.99371121\nIteration 402, loss = 1.99366837\nIteration 403, loss = 1.99367791\nIteration 404, loss = 1.99369144\nIteration 405, loss = 1.99355001\nIteration 406, loss = 1.99336726\nIteration 407, loss = 1.99385300\nIteration 408, loss = 1.99396685\nIteration 409, loss = 1.99355144\nIteration 410, loss = 1.99340697\nIteration 411, loss = 1.99322941\nIteration 412, loss = 1.99326323\nIteration 413, loss = 1.99339753\nIteration 414, loss = 1.99334313\nIteration 415, loss = 1.99355519\nIteration 416, loss = 1.99379208\nIteration 417, loss = 1.99332001\nIteration 418, loss = 1.99339894\nIteration 419, loss = 1.99315713\nIteration 420, loss = 1.99339717\nIteration 421, loss = 1.99341873\nIteration 422, loss = 1.99315184\nIteration 423, loss = 1.99339379\nIteration 424, loss = 1.99314591\nIteration 425, loss = 1.99358801\nIteration 426, loss = 1.99304880\nIteration 427, loss = 1.99314934\nIteration 428, loss = 1.99329659\nIteration 429, loss = 1.99304426\nIteration 430, loss = 1.99296586\nIteration 431, loss = 1.99313512\nIteration 432, loss = 1.99317842\nIteration 433, loss = 1.99295003\nIteration 434, loss = 1.99311410\nIteration 435, loss = 1.99288220\nIteration 436, loss = 1.99310682\nIteration 437, loss = 1.99341764\nIteration 438, loss = 1.99302585\nIteration 439, loss = 1.99315295\nIteration 440, loss = 1.99306237\nIteration 441, loss = 1.99287731\nIteration 442, loss = 1.99309259\nIteration 443, loss = 1.99323354\nIteration 444, loss = 1.99295500\nIteration 445, loss = 1.99302772\nIteration 446, loss = 1.99310052\nIteration 447, loss = 1.99285045\nIteration 448, loss = 1.99272329\nIteration 449, loss = 1.99299895\nIteration 450, loss = 1.99313472\nIteration 451, loss = 1.99311707\nIteration 452, loss = 1.99287426\nIteration 453, loss = 1.99270450\nIteration 454, loss = 1.99282054\nIteration 455, loss = 1.99274396\nIteration 456, loss = 1.99266719\nIteration 457, loss = 1.99293266\nIteration 458, loss = 1.99290504\nIteration 459, loss = 1.99265060\nIteration 460, loss = 1.99255766\nIteration 461, loss = 1.99280313\nIteration 462, loss = 1.99254940\nIteration 463, loss = 1.99265083\nIteration 464, loss = 1.99245081\nIteration 465, loss = 1.99270617\nIteration 466, loss = 1.99289990\nIteration 467, loss = 1.99251969\nIteration 468, loss = 1.99273887\nIteration 469, loss = 1.99234541\nIteration 470, loss = 1.99251672\nIteration 471, loss = 1.99231058\nIteration 472, loss = 1.99235558\nIteration 473, loss = 1.99270819\nIteration 474, loss = 1.99254384\nIteration 475, loss = 1.99232893\nIteration 476, loss = 1.99257787\nIteration 477, loss = 1.99248199\nIteration 478, loss = 1.99263889\nIteration 479, loss = 1.99226606\nIteration 480, loss = 1.99239432\nIteration 481, loss = 1.99241749\nIteration 482, loss = 1.99238199\nIteration 483, loss = 1.99242919\nIteration 484, loss = 1.99224595\nIteration 485, loss = 1.99226438\nIteration 486, loss = 1.99222245\nIteration 487, loss = 1.99260572\nIteration 488, loss = 1.99222645\nIteration 489, loss = 1.99256934\nIteration 490, loss = 1.99230286\nIteration 491, loss = 1.99229498\nIteration 492, loss = 1.99220706\nIteration 493, loss = 1.99223507\nIteration 494, loss = 1.99214966\nIteration 495, loss = 1.99221558\nIteration 496, loss = 1.99223681\nIteration 497, loss = 1.99230257\nIteration 498, loss = 1.99216510\nIteration 499, loss = 1.99229343\nIteration 500, loss = 1.99212991\n"
    }
   ],
   "source": "# PASS IN FULL FEATURE SET\nfinal_df = run.data.san_fran_df[[\n## STANDARD METRICS\n    'game_seconds_remaining',\n    'yardline_100',\n    'down',\n    'ydstogo',\n    'shotgun',\n    'score_differential',\n    'total_home_score',\n    'total_away_score',\n    'quarter_seconds_remaining',\n    'half_seconds_remaining',\n    'qtr',\n    'goal_to_go',\n    'no_huddle',\n    'posteam_timeouts_remaining',\n    'defteam_timeouts_remaining',\n## CUSTOM METRICS\n     'previous_play_in_drive',\n     'drive_yards_gained',\n     'game_yards_gained',\n     'drive_rushing_yards_gained',\n     'game_rushing_yards_gained',\n     'drive_passing_yards_gained',\n     'game_passing_yards_gained',\n     'drive_sack',\n     'game_sack',\n     'drive_incomplete_pass',\n     'game_incomplete_pass',\n     'drive_no_huddle',\n     'game_no_huddle',\n     'drive_interception',\n     'game_interception',\n     'drive_first_down_rush',\n     'game_first_down_rush',\n     'drive_first_down_pass',\n     'game_first_down_pass',\n     'drive_first_down_penalty',\n     'game_first_down_penalty',\n     'game_third_down_converted',\n     'game_third_down_failed',\n     'game_fumble',\n#      'drive_qb_hit',\n     'game_qb_hit',\n     'drive_rush_attempt',\n     'game_rush_attempt',\n     'drive_pass_attempt',\n     'game_pass_attempt',\n     'game_pass_touchdown',\n     'game_rush_touchdown',\n### GOAL METRIC\n    'full_play_type'\n]]\n\n\nfinal_df= pandas.get_dummies(data=final_df, columns=['previous_play_in_drive'])\nX = final_df.loc[:, final_df.columns != 'full_play_type']\nY = final_df.full_play_type\n\n# Encode string class Values\nlabel_encoder = LabelEncoder()\nlabel_encoder = label_encoder.fit(Y)\nlabel_encoded_y = label_encoder.transform(Y)\nseed = 13\ntest_size = 0.2\n\n# Randomizing\nX_train, X_test, y_train, y_test = \\\n    model_selection.train_test_split(X, label_encoded_y, test_size=test_size, random_state=seed)\n\n# Fit GBC model\nGBC_model = xgboost.XGBClassifier(learning_rate=0.01,n_estimators=400)\nGBC_model.fit(X_train, y_train)\n\n# Make predictions\nGBC_y_pred = GBC_model.predict(X_test)\nGBC_predictions = [round(value) for value in GBC_y_pred]\n# Evaluate Predictions\nGBC_accuracy = accuracy_score(y_test, GBC_predictions)\n\nCLF_model = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=500, alpha=0.0001,\n                     solver='sgd', verbose=10,  random_state=21,tol=0.000000001)\nCLF_model.fit(X_train, y_train)\nCLF_model_y_pred = CLF_model.predict(X_test)\nCLF_model_accuracy_score = accuracy_score(y_test, CLF_model_y_pred)\nCLF_model_cm = confusion_matrix(y_test, CLF_model_y_pred)"
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "GBC Accuracy: 28.34%\nCLF Accuracy: 18.18%\n"
    }
   ],
   "source": "print(\"GBC Accuracy: %.2f%%\" % (GBC_accuracy * 100.0))\nprint(\"CLF Accuracy: %.2f%%\" % (CLF_model_accuracy_score * 100.0))\n# print(classification_report(CLF_model_y_pred,predictions))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "list(run.data.san_fran_df.columns)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
